# Enterprise MLOps with ZenML, Docker, and MySQL <sub>(End-to-End Machine Learning Pipeline for the Iris Dataset)</sub>



## Project Overview

#### This project demonstrates an end-to-end MLOps workflow using:

- Docker for reproducible environments

- ZenML for pipeline orchestration and experiment tracking

-  MySQL as ZenML‚Äôs metadata store

- Scikit-learn for model training and evaluation

- Pandas for data handling

---

### Project Structure

```python
zenml_demo_project/
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ app/
    ‚îú‚îÄ‚îÄ .zen/                     # ZenML local repo configuration (auto-created)
    ‚îú‚îÄ‚îÄ src/
    ‚îÇ   ‚îî‚îÄ‚îÄ pipeline.py           # ZenML pipeline definition and entrypoint
    ‚îÇ
    ‚îú‚îÄ‚îÄ Dockerfile
    ‚îú‚îÄ‚îÄ requirements.txt
    ‚îú‚îÄ‚îÄ run_pipeline.sh
 ‚îú‚îÄ‚îÄ .gitignore
 ‚îú‚îÄ‚îÄ Makefile



```

--- 

## Step-by-Step Setup and Run Guide

### 1. Clone the Repository
 
```python
git clone https://github.com/manuelbomi/Enterprise-MLOps-with-ZenML-Docker-and-MySQL.git
cd zenml_mlops_docker_mysql

```


### 2. Build and Start the Docker Containers
```python
docker compose up --build
```

#### This spins up:

- A ZenML server on port 8080

- A MySQL database on port 3306

- Your app container with all dependencies installed


### 3. Enter the Application Container

```python
docker compose exec app bash
```

<img width="1280" height="720" alt="Image" src="https://github.com/user-attachments/assets/ddb5d028-209d-4a89-9b5e-eefd2977b8b7" />


<img width="1280" height="720" alt="Image" src="https://github.com/user-attachments/assets/52f521a0-c376-462f-b29c-a21a4a58ce4a" />




### 4. Log in to ZenML Server

```python
zenml login http://zenml-server:8080
```

---


<img width="1280" height="720" alt="Image" src="https://github.com/user-attachments/assets/10d57e78-256c-43f3-a565-57d9e9c6b8f7" />

---

> [!NOTE]
> If the browser link shows something like:
>
> *http://zenml-server:8080/devices/verify?device_id=...*
>
> replace zenml-server with localhost, like so:
>
> *http://localhost:8080/devices/verify?device_id=...*

---

#### Once authenticated, you will  see:

```python
Successfully logged in to http://zenml-server:8080.

```

### 5. Check ZenML Status

```python
zenml status
```

#### Expected output:
```python
Connected to a remote ZenML server: http://zenml-server:8080
Server status: available
Active user: default
Active stack: default
```


<img width="1280" height="720" alt="Image" src="https://github.com/user-attachments/assets/522eba9b-9867-4563-8377-7b0c5df44960" />


<img width="1280" height="720" alt="Image" src="https://github.com/user-attachments/assets/c4f6bcab-9141-4640-b6b9-9deb2190f016" />

### 6. (Optional) Register a Local Stack

#### If ZenML complains about missing stacks, re-register one:

```python
zenml stack register local_server_stack \
    -o default \
    -a default
zenml stack set local_server_stack
```

### 7. Run the Pipeline
```python
python src/pipeline.py
```

#### You will see logs similar to:

```python
Connected to ZenML server: http://zenml-server:8080
Active stack: default
Model accuracy: 1.0000
Pipeline run finished successfully!


```


### 8. View Pipeline Runs in the Dashboard

#### Open:

```python
http://localhost:8080
```

#### You will see the iris_training_pipeline run logged, with all steps, artifacts, and metrics.

<img width="1280" height="720" alt="Image" src="https://github.com/user-attachments/assets/ff3b6904-d9d3-4e12-aea9-29fa1b3a7826" />


<img width="1280" height="720" alt="Image" src="https://github.com/user-attachments/assets/f13a6079-d3e4-463d-95ed-4d6f1827f001" />

---

### Troubleshooting Guide

| Issue | Fix |
|-------|-----|
| Browser can‚Äôt open zenml-server | Replace with localhost in the URL. |
| Pipeline fails to connect to server | Ensure ZenML version in container matches server (`zenml==0.71.0`). |
| Version mismatch warning | Delete `.zen` and reinitialize with `zenml init`. |
| Slow data materialization | Install PyArrow: `pip install pyarrow`. |
| MySQL connection errors | Ensure zenml-db is healthy: `docker ps` ‚Üí check container logs. |


### Possible Enhancements

- Add model artifact registration (zenml.model_registry)

- Add experiment tracking with MLflow

- Deploy model via ZenML deployment stack

- Extend to use Dask / Ray orchestrators for distributed training

---
---


## Why This Project Matters for Enterprise & MLOps


#### This project is not just about ‚Äúrunning an ML model.‚Äù

#### It is essentailly a modular, containerized, and trackable ML pipeline ‚Äî exactly what enterprise teams need to move machine learning from research notebooks to production.

#### Let us break it down by MLOps capabilities and enterprise impact


### <ins>1. Reproducibility via Docker</ins>

- Same code, same environment, anywhere.

#### Why enterprises care:

#### Different engineers, OSes, or cloud environments can produce inconsistent results.

#### Docker ensures your ML pipeline, dependencies, and configurations are identical across:

- Local dev

- CI/CD runners

- Cloud deployments (AWS ECS, GCP, Azure Container Apps)
- 

#### The  project feature:


- The entire ZenML stack and pipeline run from a Docker container.

- Any engineer can reproduce results with:

```python
docker compose up --build
```

---


### <ins>2. Modular ML Pipelines with ZenML</ins>

#### A single source of truth for your ML workflow.

#### Why enterprises care:

##### ML projects often turn into spaghetti code ‚Äî data cleaning, training, and evaluation all jumbled.

##### ZenML enforces clean separation between pipeline stages:

- load_data ‚Üí train_model ‚Üí evaluate_model

- Each step can be independently tested, versioned, and replaced (e.g., swap logistic regression for XGBoost).

##### The  project feature:

- Uses ZenML‚Äôs @step and @pipeline decorators to define a DAG-like ML workflow.

- Each step produces trackable artifacts stored in the artifact store.
  

---


### <ins> 3. Experiment Tracking & Auditability</ins>

##### Know what model, data, and parameters produced every result.

##### Why enterprises care:

##### In regulated industries (finance, healthcare, insurance), reproducibility and auditing are required.

##### ZenML tracks:

- Code version

- Parameters

- Datasets

- Model files

- Accuracy metrics

##### The  project feature:

- Every pipeline run is stored and versioned in ZenML‚Äôs database.

- MLOps Engineers and other Devs can visually inspect results in the ZenML dashboard at:
  

```python
http://localhost:8080
```


---


### <ins>4. Centralized Metadata via MySQL</ins>

#### One database for all experiment and pipeline metadata.

##### Why enterprises care:

##### Data scientists, ML engineers, and DevOps need shared visibility.

##### A central SQL database makes metadata accessible for:

- Dashboards (Grafana, Superset)

- Auditing tools

- Internal MLOps analytics

##### The  project feature:

- Uses MySQL as ZenML‚Äôs metadata store (instead of the default SQLite).

- Enables multi-user access and scalability.
  

---


### <ins>5. Orchestrated ML Lifecycle</ins>

##### From raw data ‚Üí trained model ‚Üí evaluated metrics.

##### Why enterprises care:

##### A typical ML lifecycle includes:

- Data ingestion & validation

- Feature engineering

- Model training

- Evaluation

- Model deployment

- Monitoring and retraining

####  The project feature:

- Implements the first three (load, train, evaluate).

- It is structured to easily extend:

- Add a deployment step (e.g., to FastAPI, MLflow, or Seldon)

- Add a data validation step (e.g., Great Expectations)

- Integrate a CI/CD trigger (e.g., GitHub Actions)
  

---


### <ins>6. Scalability and Cloud Readiness</ins>

##### Your team may decide to deploy the same stack on Kubernetes, AWS, or GCP.

##### Why enterprises care:

##### Companies often start local, but scale pipelines in the cloud.

##### ZenML integrates with:

- Kubeflow, Airflow, or Prefect (for orchestration)

- S3, GCS, Azure Blob (for artifact stores)

- MLflow or Weights & Biases (for experiment tracking)


##### The project feature:

- Built using ZenML‚Äôs pluggable stack concept.

- You can swap out local components for cloud ones ‚Äî no code rewrite needed.

<ins>Example</ins>:

```python
zenml stack register aws_stack \
    -o kubeflow_orchestrator \
    -a s3_artifact_store \
    -m mlflow_experiment_tracker
zenml stack set aws_stack
```

---  

### <ins>7. Security and Multi-User Access</ins>

##### Supports user authentication and team collaboration.

##### Why enterprises care:

- Enterprise environments require controlled access.

- ZenML supports user authentication, API tokens, and RBAC (role-based access).

##### The project feature:

- Authenticates users via the ZenML web login flow.
- 
-  Demonstrates replacing zenml-server ‚Üí localhost for network bridging between host and container.

---

### <ins> 8. Business Impact Summary </ins>

| Business Need | How This Project Solves It |
|---------------|----------------------------|
| Model reproducibility | Docker-based environment ensures consistent builds |
| Pipeline governance | ZenML tracks artifacts, runs, and metrics |
| Multi-user collaboration | MySQL-backed server allows shared metadata |
| Experiment management | Dashboard provides run comparisons |
| Easy deployment | Docker Compose encapsulates all services |
| Scalability | ZenML's stack concept enables cloud migration |
| Security | Server authentication and credential isolation |

---

### <ins> 9. Real-World Enterprise Use Cases</ins>

| Industry | Application Example |
|----------|---------------------|
| üè¶ Finance | Credit risk scoring pipeline using ZenML orchestrator |
| üè• Healthcare | Predictive patient diagnosis with auditable ML runs |
| üöö Logistics | IoT data analytics using ZenML with Dask backend |
| üè≠ Manufacturing | Predictive maintenance models tracked via ZenML |
| üí≥ Retail | Customer churn prediction pipelines |

---

### <ins> 10. MLOps Principles Demonstrated </ins>

| MLOps Pillar | How This Project Demonstrates It |
|--------------|----------------------------------|
| Automation | Pipelines defined declaratively in ZenML |
| Reproducibility | Docker ensures same environment everywhere |
| Versioning | Runs, data, and models tracked via ZenML |
| Scalability | Pluggable architecture enables cloud scaling |
| Monitoring | Metrics logged and visualized in dashboard |
| Collaboration | Central server + database for team access |

--- 

## Summary

#### This project is an enterprise-ready MLOps starter that moves beyond simple ML scripts.

#### It aligns with the core principles of modern ML operations ‚Äî reproducibility, modularity, traceability, and scalability ‚Äî and can easily be extended to:

- Distributed training (e.g., Dask, Ray)

- Automated retraining

- Continuous deployment (CI/CD)

- Model serving APIs

- Cloud-native pipelines

---

Thank you for reading

---

### **AUTHOR'S BACKGROUND**

### Author's Name:  Emmanuel Oyekanlu
```
Skillset:   I have experience spanning several years in data science, software and AI solution design and deployments, data engineering,
high performance computing (GPU, CUDA), machine learning, MLOps, NLP, Agentic-AI and LLM applications, developing scalable enterprise data pipelines,
enterprise solution architecture, architecting enterprise systems data and AI applications, as well as deploying scalable solutions (apps) on-prem and in the cloud.

I can be reached through: manuelbomi@yahoo.com

Website:  http://emmanueloyekanlu.com/
Publications:  https://scholar.google.com/citations?user=S-jTMfkAAAAJ&hl=en
LinkedIn:  https://www.linkedin.com/in/emmanuel-oyekanlu-6ba98616
Github:  https://github.com/manuelbomi

```
[![Icons](https://skillicons.dev/icons?i=aws,azure,gcp,scala,mongodb,redis,cassandra,kafka,anaconda,matlab,nodejs,django,py,c,anaconda,git,github,mysql,docker,kubernetes&theme=dark)](https://skillicons.dev)





















